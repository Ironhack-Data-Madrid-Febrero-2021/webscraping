{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import random\n",
    "import re\n",
    "#import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-md-6\">\n",
       "<h1 class=\"h3 lh-condensed\">\n",
       "<a data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"TRENDING_DEVELOPERS_PAGE\",\"click_target\":\"OWNER\",\"click_visual_representation\":\"TRENDING_DEVELOPER\",\"actor_id\":null,\"record_id\":21002501,\"originating_url\":\"https://github.com/trending/developers\",\"user_id\":null}}' data-hydro-click-hmac=\"21e8ab4014e1cd1d6cb89ad9f2e71148ce8a868d2bf8c3031de3476fcc9da665\" data-view-component=\"true\" href=\"/an-tao\">\n",
       "            An Tao\n",
       "</a> </h1>\n",
       "<p class=\"f4 text-normal mb-1\">\n",
       "<a class=\"Link--secondary\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"TRENDING_DEVELOPERS_PAGE\",\"click_target\":\"OWNER\",\"click_visual_representation\":\"TRENDING_DEVELOPER\",\"actor_id\":null,\"record_id\":21002501,\"originating_url\":\"https://github.com/trending/developers\",\"user_id\":null}}' data-hydro-click-hmac=\"21e8ab4014e1cd1d6cb89ad9f2e71148ce8a868d2bf8c3031de3476fcc9da665\" data-view-component=\"true\" href=\"/an-tao\">\n",
       "              an-tao\n",
       "</a> </p>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = soup.find_all('div', class_=\"col-md-6\")\n",
    "name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = soup.select('.col-md-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Tao\\n \\n\\n\\n              an-tao'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[0].text.strip('\\n').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Tao (an-tao)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name[0].select('a')[0].text.strip() + ' ' + '('+name[0].select('a')[1].text.strip()+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pidonombre(name):\n",
    "    return name.select('a')[0].text.strip() + ' ' + '('+name.select('a')[1].text.strip()+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Tao (an-tao)',\n",
       " 'Mr.doob (mrdoob)',\n",
       " 'Josh Bleecher Snyder (josharian)',\n",
       " 'Chris Banes (chrisbanes)',\n",
       " 'Kyle Mathews (KyleAMathews)',\n",
       " 'Minko Gechev (mgechev)',\n",
       " 'Tobias Koppers (sokra)',\n",
       " 'ᴜɴᴋɴᴡᴏɴ (unknwon)',\n",
       " 'Alex Goodman (wagoodman)',\n",
       " 'Leo Farias (leoafarias)',\n",
       " 'Gleb Bahmutov (bahmutov)',\n",
       " 'Ross Wightman (rwightman)',\n",
       " 'Robert Mosolgo (rmosolgo)',\n",
       " 'Puru Vijay (PuruVJ)',\n",
       " 'Olivier Halligon (AliSoftware)',\n",
       " 'Kévin Dunglas (dunglas)',\n",
       " 'Earle F. Philhower, III (earlephilhower)',\n",
       " 'Greg Bergé (gregberge)',\n",
       " 'Fatih Arslan (fatih)',\n",
       " 'Jeremy Tuloup (jtpio)',\n",
       " 'Felix Angelov (felangel)',\n",
       " 'Paul Razvan Berg (paulrberg)',\n",
       " 'David Khourshid (davidkpiano)',\n",
       " 'hiroki osame (privatenumber)',\n",
       " 'Trask Stalnaker (trask)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "for i in range(0,50,2):\n",
    "    names.append(pidonombre(name[i]))\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chip-red-pill /'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_repo_py = 'https://github.com/trending/python?since=daily'\n",
    "html = requests.get(url_repo_py)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "repo_py = soup.find_all('h1', class_=\"h3 lh-condensed\")   #1º Opción\n",
    "\n",
    "repo_py[0].find('span', {\"class\":\"text-normal\"}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = soup.select ('h1 a span')   #2º Opción \n",
    "len(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chip-red-pill /'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chip-red-pill',\n",
       " 'rbignon',\n",
       " 'yuval-alaluf',\n",
       " 'TheAlgorithms',\n",
       " 'rwightman',\n",
       " 'Chia-Network',\n",
       " 'jackfrued',\n",
       " 'pandas-dev',\n",
       " 'WongKinYiu',\n",
       " 'twintproject',\n",
       " 'matrix-org',\n",
       " 'beurtschipper',\n",
       " 'plotly',\n",
       " 'facebookresearch',\n",
       " 'ddbourgin',\n",
       " 'alfonsrv',\n",
       " 'scikit-learn',\n",
       " 'mirumee',\n",
       " 'googleapis',\n",
       " 'PyTorchLightning',\n",
       " 'dgtlmoon',\n",
       " 'apache',\n",
       " 'python-poetry',\n",
       " 'google-research',\n",
       " 'pypa']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [rep.text.strip().split('/')[0].strip() for rep in repo]\n",
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_disney = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url_disney)\n",
    "soup3 = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"image\" href=\"/wiki/File:Walt_Disney_1946.JPG\"><img alt=\"Walt Disney 1946.JPG\" data-file-height=\"675\" data-file-width=\"450\" decoding=\"async\" height=\"330\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/330px-Walt_Disney_1946.JPG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/440px-Walt_Disney_1946.JPG 2x\" width=\"220\"/></a>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_href = soup3.select('a.image')\n",
    "disney_href[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_alt = soup3.select('a.image img')\n",
    "len(disney_alt), len(disney_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"image\" href=\"/wiki/File:Walt_Disney_1946.JPG\"><img alt=\"Walt Disney 1946.JPG\" data-file-height=\"675\" data-file-width=\"450\" decoding=\"async\" height=\"330\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/330px-Walt_Disney_1946.JPG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/440px-Walt_Disney_1946.JPG 2x\" width=\"220\"/></a>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_href[0]  #  en el href está el 'alt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " 'class': ['image']}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_href[20].attrs  #Quiero el link de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt': 'Flag of Los Angeles County, California.svg',\n",
       " 'src': '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'decoding': 'async',\n",
       " 'width': '30',\n",
       " 'height': '18',\n",
       " 'srcset': '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/45px-Flag_of_Los_Angeles_County%2C_California.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/60px-Flag_of_Los_Angeles_County%2C_California.svg.png 2x',\n",
       " 'data-file-width': '1522',\n",
       " 'data-file-height': '913'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_alt[20].attrs #quiero el nombre de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/File:Flag_of_Los_Angeles_County,_California.svg'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_href[20].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dis_href = [href.get('href') for href in disney_href]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dis_alt = [ alt.get('alt') for alt in disney_alt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_dis_alt),len(lista_dis_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Walt Disney 1946.JPG', '/wiki/File:Walt_Disney_1946.JPG'),\n",
       " ('Walt Disney 1942 signature.svg',\n",
       "  '/wiki/File:Walt_Disney_1942_signature.svg')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list(zip(lista_dis_alt, lista_dis_href))\n",
    "result[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walt Disney 1946.JPG</td>\n",
       "      <td>/wiki/File:Walt_Disney_1946.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walt Disney 1942 signature.svg</td>\n",
       "      <td>/wiki/File:Walt_Disney_1942_signature.svg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>/wiki/File:Walt_Disney_envelope_ca._1921.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A cartoon rabbit is driving a tramcar; other c...</td>\n",
       "      <td>/wiki/File:Trolley_Troubles_poster.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A cartoon mouse is operating a ship's steering...</td>\n",
       "      <td>/wiki/File:Steamboat-willie.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                               Walt Disney 1946.JPG   \n",
       "1                     Walt Disney 1942 signature.svg   \n",
       "2                                                      \n",
       "3  A cartoon rabbit is driving a tramcar; other c...   \n",
       "4  A cartoon mouse is operating a ship's steering...   \n",
       "\n",
       "                                              1  \n",
       "0               /wiki/File:Walt_Disney_1946.JPG  \n",
       "1     /wiki/File:Walt_Disney_1942_signature.svg  \n",
       "2  /wiki/File:Walt_Disney_envelope_ca._1921.jpg  \n",
       "3        /wiki/File:Trolley_Troubles_poster.jpg  \n",
       "4               /wiki/File:Steamboat-willie.jpg  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img_disney = pd.DataFrame(result)\n",
    "df_img_disney.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_py ='https://en.wikipedia.org/wiki/Python' \n",
    "html = requests.get(url_py)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Python_of_Catana'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_py = soup.select('li a')\n",
    "wiki_py[15].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Pythons',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlaces_py = [href['href'] for href in wiki_py if 'python' in href['href'].lower()]\n",
    "enlaces_py[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org//wiki/Pythons',\n",
       " 'https://en.wikipedia.org//wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org//wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org//wiki/Python_(painter)']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_py_full = 'https://en.wikipedia.org/'\n",
    "enlaces_py_full = [url_py_full+link for link in enlaces_py]\n",
    "enlaces_py_full[:5]   #Ahora podemos coger cualquier url que nos interese y si funciona 🤟🤟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_USA = 'http://uscode.house.gov/download/download.shtml'\n",
    "html = requests.get(url_USA)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n          Title 10 - Armed Forces ٭\\n'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_changes = soup.find_all('div', {\"class\":\"usctitlechanged\"})\n",
    "titles_changes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 10 - Armed Forces ٭']"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_changes_strong = [title.text.strip() for title in titles_changes]\n",
    "titles_changes_strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_earth = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url_earth)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td class=\"tabev6\"><b><i style=\"display:none;\">earthquake</i><a href=\"/Earthquake/earthquake.php?id=988578\">2021-05-25   08:42:20.2</a></b><i class=\"ago\" id=\"ago0\">31min ago</i></td>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = soup.find_all('td', {'class':'tabev6'})#[1].find('a').text\n",
    "dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-05-25', '2021-05-25', '2021-05-25']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = [dat.find('a').text[0:10] for dat in dates ]\n",
    "date[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08:42:20.2', '08:39:59.0', '08:17:10.9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = [tim.find('a').text[-10:] for tim in dates]\n",
    "time[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td class=\"tabev1\">32.75 </td>, <td class=\"tabev1\">115.82 </td>]\n",
      "[<td class=\"tabev2\">N  </td>, <td class=\"tabev2\">W  </td>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat = soup.find_all('td',{'class':'tabev1'})\n",
    "N_o_S_ =soup.find_all('td', {'class':'tabev2'})\n",
    "print(lat[:2]), print(N_o_S_[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['32.75', '115.82']\n",
      "['N', 'W']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude = [lati.text[:-1] for lati in lat]\n",
    "N_o_S = [i.text[0] for i in N_o_S_]\n",
    "print(latitude[:2]), print(N_o_S[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitu = []\n",
    "for lt in range(0,100,2):\n",
    "    latitu.append([latitude[lt], latitude[lt+1]])\n",
    "len(latitu)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['32.75', '115.82'],\n",
       " ['31.78', '69.99'],\n",
       " ['19.36', '155.00'],\n",
       " ['38.81', '122.82'],\n",
       " ['38.52', '29.59'],\n",
       " ['2.41', '99.24'],\n",
       " ['19.20', '155.44']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitu[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nor_sur = []\n",
    "for ns in range(0,100,2):\n",
    "    nor_sur.append([N_o_S[ns], N_o_S[ns+1]])\n",
    "len(nor_sur)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['N', 'W'],\n",
       " ['2', 'S'],\n",
       " ['W', '2'],\n",
       " ['N', 'W'],\n",
       " ['2', 'N'],\n",
       " ['W', '2'],\n",
       " ['N', 'E']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nor_sur[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td class=\"tb_region\" id=\"reg0\"> SOUTHERN CALIFORNIA</td>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = soup.find_all('td', {'class':'tb_region'})\n",
    "region[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOUTHERN CALIFORNIA', 'SAN JUAN, ARGENTINA']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_name = [city.text[1:] for city in region ]\n",
    "region_name[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'115.82'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_ = [l[0] for l in latitu]\n",
    "lon_ = [l[1] for l in latitu]\n",
    "NS_lat = str([ns[0] for ns in nor_sur])\n",
    "NS_lon = str([ns[1] for ns in nor_sur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['N', '2', 'W', 'N', '2', 'W', 'N', '2', 'E', 'N', '2', 'E', 'N', '2', 'E', 'N', '2', 'W', 'N', '2', 'E', 'S', '3', 'E', 'N', '2', 'W', 'N', '2', 'W', 'N', '2', 'W', 'N', '2', 'E', 'N', '2', 'W', 'S', '2', 'W', 'N', '2', 'W', 'N', '2', 'W', 'N', '2']\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(NS_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_ = [l+ '' + ns for l in lat_ for ns in NS_lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['32.75[',\n",
       " \"32.75'\",\n",
       " '32.75N',\n",
       " \"32.75'\",\n",
       " '32.75,',\n",
       " '32.75 ',\n",
       " \"32.75'\",\n",
       " '32.752',\n",
       " \"32.75'\",\n",
       " '32.75,']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude_[:10]   #Lo he intentado de unas cuantas formas pero no me ha salido, \n",
    "                 #seguro que es algo facilón que no puedo, pero si lo llegas a ver me lo dices porfi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50, 50, 50)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(date), len(time), len(region), len(lat_), len(lon_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Earthquake = {'Date': date, 'Time': time, 'Latitude': lat_, 'longitude': lon_, 'Region': region_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>08:42:20.2</td>\n",
       "      <td>32.75</td>\n",
       "      <td>115.82</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>08:39:59.0</td>\n",
       "      <td>31.78</td>\n",
       "      <td>69.99</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>08:17:10.9</td>\n",
       "      <td>19.36</td>\n",
       "      <td>155.00</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>08:14:29.1</td>\n",
       "      <td>38.81</td>\n",
       "      <td>122.82</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>08:10:47.7</td>\n",
       "      <td>38.52</td>\n",
       "      <td>29.59</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Time Latitude longitude                    Region\n",
       "0  2021-05-25  08:42:20.2    32.75    115.82       SOUTHERN CALIFORNIA\n",
       "1  2021-05-25  08:39:59.0    31.78     69.99       SAN JUAN, ARGENTINA\n",
       "2  2021-05-25  08:17:10.9    19.36    155.00  ISLAND OF HAWAII, HAWAII\n",
       "3  2021-05-25  08:14:29.1    38.81    122.82       NORTHERN CALIFORNIA\n",
       "4  2021-05-25  08:10:47.7    38.52     29.59            WESTERN TURKEY"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Earthquake)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*me sale este tipo de información en ambas urls que me hace desconfiar*\n",
    "\n",
    "La conexión no es privada\n",
    "Es posible que los atacantes estén intentando robar tu información de hackevents.co (por ejemplo, contraseñas, mensajes o tarjetas de crédito). Más información\n",
    "NET::ERR_CERT_COMMON_NAME_INVALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_list = 'https://www.wikipedia.org/'\n",
    "html = requests.get(url_list)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"link-box\" data-slogan=\"The Free Encyclopedia\" href=\"//en.wikipedia.org/\" id=\"js-link-box-en\" title=\"English — Wikipedia — The Free Encyclopedia\">\n",
       "<strong>English</strong>\n",
       "<small><bdi dir=\"ltr\">6 299 000+</bdi> <span>articles</span></small>\n",
       "</a>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_language = soup.find_all('a', {'class': 'link-box'})\n",
    "wiki_language[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_language[0].find('strong').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bdi dir=\"ltr\">6 299 000+</bdi>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_language[0].find('bdi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6\\xa0299\\xa0000+'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_language[0].find('bdi').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " '日本語',\n",
       " 'Español',\n",
       " 'Deutsch',\n",
       " 'Русский',\n",
       " 'Français',\n",
       " 'Italiano',\n",
       " '中文',\n",
       " 'Português',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_wiki = [lan.find('strong').text for lan in wiki_language]\n",
    "language_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6\\xa0299\\xa0000+',\n",
       " '1\\xa0268\\xa0000+',\n",
       " '1\\xa0684\\xa0000+',\n",
       " '2\\xa0576\\xa0000+',\n",
       " '1\\xa0724\\xa0000+']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_lan_wiki = [num.find('bdi').text for num in wiki_language]\n",
    "number_lan_wiki[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6299000+', '1268000+', '1684000+', '2576000+', '1724000+']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_wiki_lan = [num[0]+num[2:5]+num[6:10] for num in number_lan_wiki]\n",
    "number_wiki_lan[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '6299000+'),\n",
       " ('日本語', '1268000+'),\n",
       " ('Español', '1684000+'),\n",
       " ('Deutsch', '2576000+'),\n",
       " ('Русский', '1724000+'),\n",
       " ('Français', '2329000+'),\n",
       " ('Italiano', '1693000+'),\n",
       " ('中文', '1197000+'),\n",
       " ('Português', '1066000+'),\n",
       " ('Polski', '1473000+')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_number_languages = list(zip(language_wiki, number_wiki_lan))\n",
    "wiki_number_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_uk = 'https://data.gov.uk/'\n",
    "html = requests.get(url_uk)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"govuk-link\" href=\"/cookies\">cookies to collect information</a>,\n",
       " <a class=\"govuk-link\" href=\"/cookies\">View cookies</a>,\n",
       " <a class=\"govuk-link\" data-module=\"gem-track-click\" data-track-action=\"Cookie banner settings clicked from confirmation\" data-track-category=\"cookieBanner\" href=\"/cookies\">change your cookie settings</a>,\n",
       " <a class=\"govuk-link\" href=\"http://www.smartsurvey.co.uk/s/3SEXD/\">feedback</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Business+and+economy\">Business and economy</a>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_uk = soup.find_all('a', {'class':\"govuk-link\"})#[5].get('href')\n",
    "data_uk[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://data.gov.uk//cookies',\n",
       " 'https://data.gov.uk//cookies',\n",
       " 'https://data.gov.uk//cookies',\n",
       " 'https://data.gov.uk/http://www.smartsurvey.co.uk/s/3SEXD/',\n",
       " 'https://data.gov.uk//search?filters%5Btopic%5D=Business+and+economy',\n",
       " 'https://data.gov.uk//search?filters%5Btopic%5D=Crime+and+justice',\n",
       " 'https://data.gov.uk//search?filters%5Btopic%5D=Defence']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_url = 'https://data.gov.uk/'\n",
    "data_uk_link = [uk_url+href.get('href') for href in data_uk]\n",
    "data_uk_link[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cookies to collect information',\n",
       " 'View cookies',\n",
       " 'change your cookie settings',\n",
       " 'feedback',\n",
       " 'Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_uk_text = [content.text for content in data_uk]\n",
    "data_uk_text[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>data_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and economy</td>\n",
       "      <td>https://data.gov.uk//search?filters%5Btopic%5D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime and justice</td>\n",
       "      <td>https://data.gov.uk//search?filters%5Btopic%5D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defence</td>\n",
       "      <td>https://data.gov.uk//search?filters%5Btopic%5D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>https://data.gov.uk//search?filters%5Btopic%5D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment</td>\n",
       "      <td>https://data.gov.uk//search?filters%5Btopic%5D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                          data_link\n",
       "0  Business and economy  https://data.gov.uk//search?filters%5Btopic%5D...\n",
       "1     Crime and justice  https://data.gov.uk//search?filters%5Btopic%5D...\n",
       "2               Defence  https://data.gov.uk//search?filters%5Btopic%5D...\n",
       "3             Education  https://data.gov.uk//search?filters%5Btopic%5D...\n",
       "4           Environment  https://data.gov.uk//search?filters%5Btopic%5D..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_data = {'title': data_uk_text[4:], 'data_link': data_uk_link[4:]}\n",
    "df = pd.DataFrame(uk_data)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_languages = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html= requests.get(url_languages)\n",
    "soup= BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = soup.find_all(\"table\")\n",
    "#top[3]   ES LA TABLA TOP LANGUAGES  que vamos buscando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ = top[3]\n",
    "top_.find_all('a')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_languages = []\n",
    "for f in top_.find_all('tr'):\n",
    "    fila = [celda for celda in f.find_all('td')]\n",
    "    if len(fila) > 0:\n",
    "        top_lan = fila[1].find('a')\n",
    "        \n",
    "        top_languages.append(top_lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mandarin'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = top_languages[:11]\n",
    "b[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Punjabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   top_languages\n",
       "0       Mandarin\n",
       "1        Spanish\n",
       "2        English\n",
       "3          Hindi\n",
       "4         Arabic\n",
       "5     Portuguese\n",
       "6        Bengali\n",
       "7        Russian\n",
       "8       Japanese\n",
       "9        Punjabi\n",
       "10        German"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [top.text for top in b]\n",
    "d ={'top_languages': c}\n",
    "top_languages = pd.DataFrame(d)\n",
    "top_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url_IMDB = 'https://www.imdb.com/chart/top'\n",
    "html = requests.get(url_IMDB)\n",
    "soup= BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cómo entrenar a tu dragón'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name = soup.find_all('td', {'class':'titleColumn'})\n",
    "movie_name[204].text.strip()[4:].strip()[:-6].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Cadena perpetua',\n",
       "  'El padrino',\n",
       "  'El padrino: Parte II',\n",
       "  'El caballero oscuro',\n",
       "  '12 hombres sin piedad'],\n",
       " 250)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = [name.text.strip()[4:].strip()[:-6].strip() for name in movie_name]\n",
    "name[:5], len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2010)'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_year = soup.find_all('td', {'class':'titleColumn'})\n",
    "movie_year[204].text.strip()[-6:]#.strip()[:-6].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(1994)', '(1972)', '(1974)', '(2008)', '(1957)'], 250)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_release = [year.text.strip()[-6:] for year in movie_year]\n",
    "initial_release[:5], len(initial_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dean DeBlois (dir.), Jay Baruchel, Gerard Butler'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "director = soup.find_all('td', {'class':'titleColumn'})\n",
    "director[204].find('a').get('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Frank Darabont (dir.), Tim Robbins, Morgan Freeman',\n",
       "  'Francis Ford Coppola (dir.), Marlon Brando, Al Pacino',\n",
       "  'Francis Ford Coppola (dir.), Al Pacino, Robert De Niro',\n",
       "  'Christopher Nolan (dir.), Christian Bale, Heath Ledger',\n",
       "  'Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb'],\n",
       " 250)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_director = [direc.find('a').get('title') for direc in director]\n",
    "movie_director[:5], len(movie_director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.1'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = soup.find_all('td', {'class':'ratingColumn imdbRating'})\n",
    "director[204].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.2', '9.1', '9.0', '9.0', '8.9']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_stars = [rating.text.strip() for rating in stars]\n",
    "movie_stars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>year</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Frank Darabont (dir.), Tim Robbins, Morgan Fre...</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>Francis Ford Coppola (dir.), Marlon Brando, Al...</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>Francis Ford Coppola (dir.), Al Pacino, Robert...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>Christopher Nolan (dir.), Christian Bale, Heat...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>La princesa prometida</td>\n",
       "      <td>(1987)</td>\n",
       "      <td>Rob Reiner (dir.), Cary Elwes, Mandy Patinkin</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Amanecer</td>\n",
       "      <td>(1927)</td>\n",
       "      <td>F.W. Murnau (dir.), George O'Brien, Janet Gaynor</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>(2013)</td>\n",
       "      <td>Jeethu Joseph (dir.), Mohanlal, Meena</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Neon Genesis Evangelion: The End of Evangelion</td>\n",
       "      <td>(1997)</td>\n",
       "      <td>Hideaki Anno (dir.), Megumi Ogata, Megumi Haya...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>El hombre que mató a Liberty Valance</td>\n",
       "      <td>(1962)</td>\n",
       "      <td>John Ford (dir.), James Stewart, John Wayne</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         movie_name    year  \\\n",
       "0                                   Cadena perpetua  (1994)   \n",
       "1                                        El padrino  (1972)   \n",
       "2                              El padrino: Parte II  (1974)   \n",
       "3                               El caballero oscuro  (2008)   \n",
       "4                             12 hombres sin piedad  (1957)   \n",
       "..                                              ...     ...   \n",
       "245                           La princesa prometida  (1987)   \n",
       "246                                        Amanecer  (1927)   \n",
       "247                                        Drishyam  (2013)   \n",
       "248  Neon Genesis Evangelion: The End of Evangelion  (1997)   \n",
       "249            El hombre que mató a Liberty Valance  (1962)   \n",
       "\n",
       "                                              director stars  \n",
       "0    Frank Darabont (dir.), Tim Robbins, Morgan Fre...   9.2  \n",
       "1    Francis Ford Coppola (dir.), Marlon Brando, Al...   9.1  \n",
       "2    Francis Ford Coppola (dir.), Al Pacino, Robert...   9.0  \n",
       "3    Christopher Nolan (dir.), Christian Bale, Heat...   9.0  \n",
       "4        Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb   8.9  \n",
       "..                                                 ...   ...  \n",
       "245      Rob Reiner (dir.), Cary Elwes, Mandy Patinkin   8.0  \n",
       "246   F.W. Murnau (dir.), George O'Brien, Janet Gaynor   8.0  \n",
       "247              Jeethu Joseph (dir.), Mohanlal, Meena   8.0  \n",
       "248  Hideaki Anno (dir.), Megumi Ogata, Megumi Haya...   8.0  \n",
       "249        John Ford (dir.), James Stewart, John Wayne   8.0  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'movie_name':name, 'year':initial_release, 'director':movie_director, 'stars':movie_stars}\n",
    "df = pd.DataFrame(dic)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
